{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering simulated sequences using TMRCA\n",
    "\n",
    "Name: CQS21 \n",
    "FYP 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing packages to my python environment\n",
    "# NOTE: my pip is v22.0.4 but I cant seem to update to the newest 24.0\n",
    "\n",
    "#!pip install zarr\n",
    "#!pip install scipy\n",
    "#!pip install scikit-allel\n",
    "#!pip install matplotlib\n",
    "#!pip install tqdm\n",
    "#!pip install dask\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import zarr\n",
    "#import allel   \n",
    "#idk why but my allel doesnt seem to load?? even though ive definitely installed it as scikit-allel\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.spatial\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask import compute, delayed\n",
    "from itertools import combinations\n",
    "import time\n",
    "import seaborn as sns\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Generate burn in simulations establishing nucleotide diversity across a range of parameters (DONE BY PREVIOUS STUDENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use HPC\n",
    "1. SHELL SCRIPT for HPC BELOW #run test for just 1 burn in\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "!/bin/bash\n",
    "PBS -N run_pairwise    ###REMOVE ME -  Change this to set the name of your script (can i set a script in SLIM?)\n",
    "PBS -j oe\n",
    "PBS -k oe             \n",
    "\n",
    "PBS -m ae\n",
    "\n",
    "PBS -l walltime=48:00:00\n",
    "PBS -l select=1:ncpus=4:mem=5gb          #job parameters update after trial\n",
    "                                  \n",
    "module load anaconda3                     #loading modules: what modules do I need? is there a module for slim?\n",
    "source activate f2                       # what does this f2 mean?\n",
    "\n",
    "\n",
    "## to direct output to cwd, use $PBS_O_WORKDIR:\n",
    "## specify LOGFILE found in ~/ during execution then moved to cwd on job completion\n",
    "##\n",
    "cd $PBS_O_WORKDIR\n",
    "JOBNUM=`echo $PBS_JOBID | sed 's/\\..*//'`\n",
    "LOGFILE=${PBS_JOBNAME}.o${JOBNUM}\n",
    "\n",
    "#########################################\n",
    "##                                     ##\n",
    "## Output some useful job information. ##\n",
    "##                                     ##\n",
    "#########################################\n",
    "\n",
    "echo ------------------------------------------------------\n",
    "echo -n 'Job is running on node '; cat $PBS_NODEFILE\n",
    "echo ------------------------------------------------------\n",
    "echo PBS: qsub is running on $PBS_O_HOST\n",
    "echo PBS: originating queue is $PBS_O_QUEUE\n",
    "echo PBS: executing queue is $PBS_QUEUE\n",
    "echo PBS: working directory is $PBS_O_WORKDIR\n",
    "echo PBS: execution mode is $PBS_ENVIRONMENT\n",
    "echo PBS: job identifier is $PBS_JOBID\n",
    "echo PBS: job name is $PBS_JOBNAME\n",
    "echo PBS: job number is $JOBNUM\n",
    "echo PBS: logfile is $LOGFILE\n",
    "echo PBS: node file is $PBS_NODEFILE\n",
    "echo PBS: current home directory is $PBS_O_HOME\n",
    "#echo PBS: PATH = $PBS_O_PATH\n",
    "echo ------------------------------------------------------\n",
    "\n",
    "\n",
    "# move LOGFILE to cwd\n",
    "##\n",
    "mv $HOME/$LOGFILE $PBS_O_WORKDIR\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "2. submit shell script using resource manager\n",
    "qsub slim_simulations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##HPC example run script from Josh Reynolds\n",
    "#!/bin/bash\n",
    "\n",
    "#PBS -N run_pairwise    ###REMOVE ME -  Change this to set the name of your script\n",
    "#PBS -j oe\n",
    "#PBS -k oe\n",
    "\n",
    "#PBS -m ae\n",
    "\n",
    "#PBS -l walltime=72:00:00\t###REMOVE ME - change this to set your time (hh:mm:ss)\n",
    "#PBS -l select=1:ncpus=4:mem=20gb       ###REMOVE ME - leave select at 1, choose your cpus and memory\n",
    "\n",
    "## NB values for ncpus and mem are allocated\n",
    "## to each node (specified by select=N)\n",
    "##\n",
    "## to direct output to cwd, use $PBS_O_WORKDIR:\n",
    "## specify LOGFILE found in ~/ during execution then moved to cwd on job completion\n",
    "##\n",
    "cd $PBS_O_WORKDIR\n",
    "JOBNUM=`echo $PBS_JOBID | sed 's/\\..*//'`\n",
    "LOGFILE=${PBS_JOBNAME}.o${JOBNUM}\n",
    "\n",
    "#########################################\n",
    "##                                     ##\n",
    "## Output some useful job information. ##\n",
    "##                                     ##\n",
    "#########################################\n",
    "\n",
    "echo ------------------------------------------------------\n",
    "echo -n 'Job is running on node '; cat $PBS_NODEFILE\n",
    "echo ------------------------------------------------------\n",
    "echo PBS: qsub is running on $PBS_O_HOST\n",
    "echo PBS: originating queue is $PBS_O_QUEUE\n",
    "echo PBS: executing queue is $PBS_QUEUE\n",
    "echo PBS: working directory is $PBS_O_WORKDIR\n",
    "echo PBS: execution mode is $PBS_ENVIRONMENT\n",
    "echo PBS: job identifier is $PBS_JOBID\n",
    "echo PBS: job name is $PBS_JOBNAME\n",
    "echo PBS: job number is $JOBNUM\n",
    "echo PBS: logfile is $LOGFILE\n",
    "echo PBS: node file is $PBS_NODEFILE\n",
    "echo PBS: current home directory is $PBS_O_HOME\n",
    "#echo PBS: PATH = $PBS_O_PATH\n",
    "echo ------------------------------------------------------\n",
    "\n",
    "## load common modules as standard\n",
    "##\n",
    "module load anaconda3/personal vcftools   ### load any modules you need here (e.g. vcftools, ibdseq). To see a list of available modules, use \"module avail\" on your login node\n",
    "source activate f2\n",
    "\n",
    "## command timed to get mem and wallclock info\n",
    "##\n",
    "\n",
    "R --vanilla --quiet --slave < ~/f2/scripts/pairwise_dif.R   ### Then supply your command. Either write the code here, or in a separate shell script that you then call on here.\n",
    "\n",
    "## move LOGFILE to cwd\n",
    "##\n",
    "mv $HOME/$LOGFILE $PBS_O_WORKDIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SLIM code file burnin_no.1_read.slim\n",
    "\n",
    "initialize() {\n",
    "\n",
    "\n",
    "        // Use tree-sequence recording to speed up burn-in\n",
    "        initializeTreeSeq();\n",
    "\n",
    "        defineConstant('pop_size',100);\n",
    "        //defineConstant('T_N',10*pop_size);\n",
    "        defineConstant('ge_length',70000);\n",
    "        defineConstant('sweep_site',integerDiv((ge_length+1),2));\n",
    "        defineConstant('low_site',sweep_site-1);\n",
    "        defineConstant('high_site',1+sweep_site);\n",
    "        \n",
    "\n",
    "        defineConstant('recombination_rate',0.00025);\n",
    "        defineConstant('mut_rate',0.00025);\n",
    "        //defineConstant('T_mu', 10/mut_rate); //round it\n",
    "        defineConstant('nucleotide_diversity', (4*pop_size*mut_rate)/(1+(2*4*pop_size*mut_rate)));\n",
    "        initializeRecombinationRate(recombination_rate);\n",
    "        initializeMutationRate(mut_rate);\n",
    "\n",
    "        // m1 is a neutral mutation\n",
    "        initializeMutationType('m0', 0.5, 'f', 0.0);\n",
    "        m0.mutationStackPolicy = \"l\";   \n",
    "\n",
    "        initializeGenomicElementType('g1', m0, 1.0 );\n",
    "        initializeGenomicElement(g1, 0, low_site);\n",
    "        initializeGenomicElement(g1, sweep_site, sweep_site);\n",
    "        initializeGenomicElement(g1, high_site, ge_length);}\n",
    "\n",
    "\n",
    "1 early(){\n",
    "\tsim.addSubpop(\"p0\", pop_size);\n",
    "}\n",
    "//this command was later added to various files on the high-performance computer.\n",
    "1:1000 late(){\n",
    "\tdiv = calcHeterozygosity(p0.genomes);\n",
    "\tcatn(sim.cycle +\",\"+ div);\n",
    "}\n",
    "       \n",
    "//conditional termination of somulation based on val of nucleotide diversity\n",
    "500:39999 late() {\n",
    "div = calcHeterozygosity(p0.genomes);\n",
    "if(div >= nucleotide_diversity*1.1){ // if(div >= nucleotide_diversity){\n",
    "sim.simulationFinished();\n",
    "catn('number of generations taken ' + sim.cycle);\n",
    "sim.treeSeqOutput(\"burnin_no.1_early_10.trees\");\n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "// Run burn-in for ~10/mu generations\n",
    "40000 late() {\n",
    "        // Output recorded tree-sequence\n",
    "        sim.treeSeqOutput(\"burnin_no.1._10_.trees\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Conduct 27 variations of partial soft sweep simulations and output vcf files using the HPC facility. Complete x number of repeats.\n",
    "\n",
    "Size of population, N: 100, 1000, 10000\n",
    "\n",
    "Mutation rate, Î¼ : (0.1/4N), (1/4N), (5/4N)\n",
    "\n",
    "Recombination rate, r : (0.1/4N), (1/4N), (5/4N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define parameter values\n",
    "N_values = [100, 1000, 10000]\n",
    "mutation_rates = [0.1 / (4 * N) for N in N_values]\n",
    "recombination_rates = [0.1 / (4 * N) for N in N_values]\n",
    "\n",
    "# Generate combinations of parameters\n",
    "parameter_combinations = [(N, mu, r) for N in N_values for mu in mutation_rates for r in recombination_rates]\n",
    "\n",
    "# Ensure we have exactly 27 combinations\n",
    "assert len(parameter_combinations) == 27, \"Number of combinations must be 27\"\n",
    "\n",
    "# Assign file numbers to combinations\n",
    "burn_in_files = list(range(1, 28))  # List of file numbers from 1 to 27\n",
    "combinations_matrix = np.zeros(27, dtype=[('N', int), ('mu', float), ('r', float), ('file', int)])\n",
    "for index, (N, mu, r) in enumerate(parameter_combinations):\n",
    "    combinations_matrix[index] = (N, mu, r, burn_in_files[index])\n",
    "\n",
    "# Save the matrix to a text file\n",
    "np.savetxt(\"parameter_combinations.txt\", combinations_matrix, fmt=\"%d,%.6f,%.6f,%d\", delimiter=\",\",\n",
    "           header=\"N, mu, r, file\", comments=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN \n",
    "1. SHELL SCRIPT for HPC BELOW\n",
    "\n",
    "!/bin/bash\n",
    "PBS -l walltime=24:00:00\n",
    "PBS -l select=1:ncpus=30:mem=60gb          #job parameters update after trial\n",
    "PBS -J 1-10                                  #array job for 27 combinations?\n",
    "module load    ????                      #loading modules: what modules do I need? is there a module for slim?\n",
    "cd $PBS_O_WORKDIR\n",
    "slim_simulations.py                                 # my script in python\n",
    "\n",
    "\n",
    "2. submit shell script using resource manager\n",
    "qsub slim_simulations.py\n",
    "\n",
    "\n",
    "\n",
    "##PBS -J n-m\n",
    "\n",
    "#in your jobscript, where n and m are integers, e.g 1-4 if you wanted four runs. \n",
    "#You can then supply these to your slim command using the variable $PBS_ARRAY_INDEX with this taking on the value of J. \n",
    "#You'll then need something in your slim code to interpret and convert these values.\n",
    "\n",
    "\n",
    "\n",
    "# choosing a wall time \n",
    "# Sensible values are 8, 24 hours, 48 hours or 72 hours. There is little benefit in selecting intermediate values.\n",
    "#If possible, use checkpointing, i.e. save intermediate results that can be used to restart your program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python script for matrix iteration? with array run\n",
    "\n",
    "#The code uses $PBS_ARRAY_INDEX to execute the correct iteration of the loop that processes just one element of the matrix.\n",
    "#All array runs combined process all elements of the matrix in parallel.\n",
    "\n",
    "\n",
    "#slide 97 of the Intro to HPC wiki pdf\n",
    "input = np.loadtxt(\"parameter_combinations.txt\", dtype='f', delimiter=',')\n",
    "counter = 0\n",
    "array_index = int(os.environ['PBS_ARRAY_INDEX'])\n",
    "for i in range(0,input.shape[0]):\n",
    "    for j in range(0, input.shape[1]):\n",
    "        counter = counter+1\n",
    "        if counter == array_index:\n",
    "            print('processing element', input[i,j])\n",
    "            # your code goes in here\n",
    "            \n",
    "            # how to get it to run in SLIM???\n",
    "\n",
    "            #SLIM Script below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Read in VCF Files from soft sweep simulations, extract haplotypes using scikit_allel\n",
    "\n",
    "Working with 27 simulation variants and repeats of each variant (100?)\n",
    "\n",
    "but why only look at haplotypes of first 200 individuals? and why calculate sample frequency over specifically 400 haplotypes? is it linked to the 200 individuals = 400 genomes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = sys.argv[1]\n",
    "#prev student: Like Hamming distance code, this was also taken from Anushka Thawani. Adaptations were made to this on the \n",
    "#high-performance computer using shell script, but this could not be represented.\n",
    "def convert(file, genome):\n",
    "    '''\n",
    "    This function extracts haplotypes sequences from a vcf file \n",
    "    Adapted from: http://alimanfoo.github.io/2018/04/09/selecting-variants.html \n",
    "    \n",
    "    Arguments:\n",
    "        file: name of vcf file (from SLiM soft sweep simulation)\n",
    "        genome: length of genome used in SLiM simulation \n",
    "        \n",
    "    Returns:\n",
    "        ht: haplotype sequences for 200 individuals\n",
    "        samp_freq: frequency of sweep mutation in sample\n",
    "        cols: used to color dendrogram\n",
    "\n",
    "    '''\n",
    "    \n",
    "    v = file + '.vcf'\n",
    "    z = file + '.zarr'\n",
    "    slim_sim_data = allel.read_vcf(v, fields='*')\n",
    "    allel.vcf_to_zarr(v, z, fields='*', overwrite=True)\n",
    "    data = zarr.open_group(z, mode='r')\n",
    "    \n",
    " \n",
    "    pos = allel.SortedIndex(data['variants/POS']) # Stores the ID and genomic position of each variant\n",
    "    \n",
    "    # Extract genotypes for the first 200 individuals and convert to haplotypes\n",
    "    gt = data['calldata/GT'][:,0:200] \n",
    "    ht = allel.GenotypeArray(gt).to_haplotypes()\n",
    "    \n",
    "    mutation = int((genome+1)/2) + 1  # position of sweep mutation\n",
    "    \n",
    "    \n",
    "    # Output the frequency of the sweep mutation in the sample\n",
    "    contains_sweep = pos.locate_range(mutation,mutation)\n",
    "    sweep = ht[contains_sweep]\n",
    "    sweep = np.sum(sweep, axis =0)\n",
    "    \n",
    "    samp_freq = np.sum(sweep)/400  # 400 haplotypes\n",
    "    \n",
    "    \n",
    "    # This dictionary is used later to color the dendrogram branches according to whether or not the \n",
    "    # corresponding sequence contains the sweep mutation\n",
    "    cols = {}\n",
    "    for i in range(400):\n",
    "        if sweep[i]:\n",
    "            cols[i] = 'r'  \n",
    "        else:\n",
    "            cols[i] = \"#808080\"\n",
    "    \n",
    "    return ht, pos, samp_freq, cols, sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Calculate Hij (S or homozygosity) for all pairs of haplotypes\n",
    "make a function \n",
    "homozygosity = (no. of SNPs/ length of window, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_homozygosity (length, genome, ht, sweeploc, pos):\n",
    "    '''\n",
    "    This function calculates the sliding homozygosity for all pairs of haplotypes.\n",
    "    \n",
    "    Arguments:\n",
    "    length: length of sliding window\n",
    "    genome : length of genome \n",
    "    ht : vector?  of haplotype sequences from convert() function in previous codeblock\n",
    "    pos: position of variants from convert() function in previous codeblock\n",
    "    sweeploc: position of sweep mutation in genome\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    homozygosities:  homozygosity of all haplotypes in ht in a array(?)\n",
    "    '''\n",
    "    # Make empty vectors\n",
    "    homozygosities = []\n",
    "    \n",
    "    #iterate through all nucleotides in the genome\n",
    "    for x in range(0,genome):\n",
    "        #define nt ranges \n",
    "        start  = x\n",
    "        end = x + length\n",
    "        # locate the position of region around a variant Nt\n",
    "        region = pos.locate_range(start,end) \n",
    "        haplotype_region = ht [region]\n",
    "\n",
    "        #use allel.pairwise distance\n",
    "        pairwise_dist = allel.pairwise_distance(haplotype_region, metric = 'hamming' , chunked=True, blen=None) #can i chunk = true to speed up computation? \n",
    "        homozygosities = pairwise_dist/length\n",
    "\n",
    "    return homozygosities\n",
    "\n",
    "\n",
    "\n",
    "## TO DO streamlining: previous students code say to add a if condition (region =/= prev region to speed up code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Calculate Lij (shared haplotype length) from Hij by finding width at half maximum homozygosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#half_max_homozygosity = (max(homozygosities) + min(homozygosities))/2\n",
    "threshold = 0.87  #setting threshold to calculate trough points in each haplotype\n",
    "\n",
    "#Defining function to find troughs, to be used in calculating Lij in another function\n",
    "def finding_troughs(smooth, sweeploc):\n",
    "    '''\n",
    "    This function finds troughs for a pair of haplotype sequences\n",
    "    \n",
    "    Arguments:\n",
    "        sliding homozygosity: smoothed sliding window homozygosity for all pairs of sequences\n",
    "        sweeploc: position of sweep mutation in genome (same as previous function)\n",
    "        \n",
    "    Returns:\n",
    "        lower: position of breakpoint left of the sweep site\n",
    "        upper: position of breakpoint right of the sweep site\n",
    "        SHL: shared haplotype length\n",
    "    '''\n",
    "    \n",
    "    #finding troughs\n",
    "    troughs = scipy.signal.find_peaks(-smooth) #- sign inverts the graph so the 'peaks' are our troughs\n",
    "    troughs = troughs[0]     # Indexes of all troughs\n",
    "    troughs = troughs[smooth[troughs] < threshold]   # Extract troughs where homozygosity<threshold\n",
    "    \n",
    "    #finds the peaks\n",
    "    peaks = scipy.signal.find_peaks(smooth)\n",
    "    peaks = peaks[0]  #indexes peaks\n",
    "    \n",
    "    # Find positions of troughs flanking sweep site\n",
    "    bp = np.searchsorted(troughs,sweeploc)  #search sorted finds index of position where mutation should be inserted in order to maintain the same order\n",
    "    lower = troughs[bp - 1] #index of sweep site -1\n",
    "    upper = troughs[bp]  #index of sweep site\n",
    "    \n",
    "    # Find the average peak position around the sweep site\n",
    "    highest = peaks[(peaks >= lower) & (peaks <= upper)]\n",
    "    if highest.size != 0:\n",
    "        highest = np.mean(highest)\n",
    "    else: \n",
    "        highest = (lower+upper)/2\n",
    "    \n",
    "    \n",
    "    lower = (lower+highest)/2\n",
    "    upper = (upper+highest)/2\n",
    "\n",
    "    SHL = upper - lower\n",
    "    \n",
    "    return int(lower), int(upper), SHL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function using each haplotype pair to return SHL and find lower and upper limits of SHL\n",
    "# uses finding_troughs()\n",
    "def find_breakpoint(haplotype_pair):\n",
    "    '''\n",
    "    For a pair of sequences, this function smoothes the sliding homozygosity and returns the SHL\n",
    "    Arguments:\n",
    "        haplotype_pair: a pair of haplotype sequences\n",
    "        \n",
    "    Returns:\n",
    "        lower: position of breakpoint left of the sweep site\n",
    "        upper: position of breakpoint right of the sweep site\n",
    "        SHL: shared haplotype length\n",
    "    '''\n",
    "    \n",
    "    mutation_pos = mutation \n",
    "    smooth = gaussian_filter1d(haplotype_pair, points_g)\n",
    "    try:\n",
    "        lower, upper, SHL = finding_troughs(smooth, mutation_pos)\n",
    "    except IndexError:\n",
    "        lower = -1.3\n",
    "        upper = -1.3\n",
    "        SHL = -1.3\n",
    "        \n",
    "    return lower, upper, SHL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6. Calculate Tij (Time to common ancestor) from Lij and Kij (which is no. of SNPs) on each shared length (haplotype)\n",
    "\n",
    "ð_ðð=(ð_ðð+1)/(2â_ðð (ð+ð))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Kij (No. of SNPs in each SHL)\n",
    "\n",
    "def calculating_kij(n,gts,ht,result_find_breakpoint,pos):\n",
    "    '''\n",
    "    This function finds the number of SNPs over the shared haplotype length for all pairs of haplotype sequences\n",
    "    \n",
    "    Arguments:\n",
    "        n: number of haplotype sequences\n",
    "        gts: number of haplotype pairs\n",
    "        ht: haplotype sequnces\n",
    "        result_find_breakpoint: output from find_breakpoint function\n",
    "        \n",
    "    Returns:\n",
    "        diffs: number of SNPs for all pairs of haplotype sequences\n",
    "        \n",
    "    '''\n",
    "    pairwise = []\n",
    "    for combo in combinations(list(range(0,n)), 2): \n",
    "        pairwise.append(combo)\n",
    "\n",
    "    diffs = np.empty(shape=(gts),dtype=np.float32)\n",
    "    for i in range(gts):\n",
    "        pair = ht[:,pairwise[i]]\n",
    "        try:\n",
    "            start = result_find_breakpoint[i,1]\n",
    "            stop = result_find_breakpoint[i,2]\n",
    "\n",
    "            window_pos = pos.locate_range(start, stop)\n",
    "            window = pair[window_pos]\n",
    "\n",
    "            d = allel.pairwise_distance(window, metric = \"hamming\")\n",
    "\n",
    "            diffs[i]=d \n",
    "\n",
    "        except KeyError:\n",
    "            diffs[i]=-1.3 \n",
    "    \n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Tij, Time to Common Ancestor (TMRCA) using mutation rate and number of SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREV STUDENTS SCRIPT FOR REFERENCE\n",
    "# delete after doing my own\n",
    "\n",
    "\n",
    "def analysis(file,genome,pop,window,threshold,points,r=1,u=1): \n",
    "    '''\n",
    "    This function clusters the sequences stored in a .vcf file.\n",
    "    \n",
    "    Arguments:\n",
    "        file: name of vcf file\n",
    "        genome: length of genome (in SLiM simulation)\n",
    "        pop: effective population size (in SLiM simulation)\n",
    "        window: length of sliding window\n",
    "        threshold: threshold above which troughs are ignored\n",
    "        points: number of points to use for 1D-gaussian filter (see scipy documentation)\n",
    "        r: recombination rate\n",
    "        u: mutation rate \n",
    "    '''\n",
    "    \n",
    "    print(file)\n",
    "\n",
    "    global mutation\n",
    "    mutation = int((genome+1)/2) \n",
    "    global thresh\n",
    "    thresh=threshold\n",
    "    global points_g\n",
    "    points_g = points\n",
    "    \n",
    "    # Extract haplotype sequences from .vcf file\n",
    "    ht, pos, samp_freq, cols, sweep = convert(file, genome)\n",
    "\n",
    "    \n",
    "    # Calculate sliding homozygosity for all pairs of haplotype sequences\n",
    "    L=window\n",
    "    n = 400 #number of haplotypes \n",
    "    gts = int((n*(n-1))/2)\n",
    "    hom = sliding_distance(L, mutation, genome, ht, pos, gts)\n",
    "\n",
    "    \n",
    "    # Find SHL for all pairs of haplotype sequences \n",
    "    hom_dask = dask.array.from_array(hom, chunks=(genome,1)) \n",
    "    hom = []\n",
    "    results = dask.array.apply_along_axis(find_breakpoint,0,hom_dask) \n",
    "    results_computed = results.compute()\n",
    "\n",
    "    # Manipulating the dataframe to make it easier to process\n",
    "    results_computed = np.transpose(results_computed)\n",
    "    index = np.asarray(range(0,gts))\n",
    "    index = np.expand_dims(index, axis=0)\n",
    "    results_computed_1 = np.concatenate((index.T, results_computed), axis=1)\n",
    "    \n",
    "    \n",
    "    # Calculate the TMRCA from the SHLs and number of SNPs\n",
    "    r = r/(2*pop)\n",
    "    u = u/(100*pop)\n",
    "    shls = results_computed_1[:,3]   # SHLs for all pairs of haplotype sequences \n",
    "    shls[shls<=0] = genome\n",
    "    diffs = find_snp(n,gts,ht,results_computed_1,pos)  # SNPs for all pairs of haplotype sequences \n",
    "    snp = (1+(diffs*shls))/(2*shls*(r + u)) # TMRCA metric for all pairs of haplotype sequences \n",
    "\n",
    "    \n",
    "    # Remove negative and non-integer TMRCA values\n",
    "    impute = np.nanmean(snp)\n",
    "    x = np.isfinite(snp)\n",
    "    for i in np.where(x == 0)[0]:\n",
    "        snp[i] = impute\n",
    "    snp[snp<=0] = impute  \n",
    "    \n",
    "    \n",
    "    # Clustering \n",
    "    Z = sch.linkage(snp, method = 'complete')\n",
    "    \n",
    "    \n",
    "    # Plot dendrogram without colouring branches\n",
    "    matplotlib.rcParams.update({'font.size': 24})\n",
    "    fig = plt.figure(figsize=(30, 12))\n",
    "    gs = matplotlib.gridspec.GridSpec(2, 1, hspace=0.1, wspace=1, height_ratios=(1,1))\n",
    "\n",
    "    ax_dend = fig.add_subplot(gs[0, 0])\n",
    "    sns.despine(ax=ax_dend, offset=5, bottom=True, top=True)\n",
    "    dd = sch.dendrogram(Z,color_threshold=0,above_threshold_color='#808080',ax=ax_dend)\n",
    "\n",
    "    ls = []\n",
    "    for leaf, leaf_color in zip(plt.gca().get_xticklabels(), dd[\"leaves_color_list\"]):\n",
    "        leaf.set_color(cols[int(leaf.get_text())])\n",
    "        ls.append(int(leaf.get_text()))\n",
    "\n",
    "    ax_dend.set_ylabel('Haplotype age/generations',fontsize=24)\n",
    "    ax_dend.set_title('Haplotype clusters',fontsize=24)\n",
    "    \n",
    "    \n",
    "    # Plot dendrogram and colour branches\n",
    "    \n",
    "    ax_dend_2 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    dflt_col = \"#808080\"\n",
    "    \n",
    "    link_cols = {}\n",
    "    for i, i12 in enumerate(Z[:,:2].astype(int)):\n",
    "        c1, c2 = (link_cols[x] if x > len(Z) else cols[x] for x in i12)\n",
    "        link_cols[i+1+len(Z)] = c1 if c1 == c2 else dflt_col\n",
    "\n",
    "    sns.despine(ax=ax_dend_2, offset=5, bottom=True, top=True)\n",
    "    dd = sch.dendrogram(Z,color_threshold=None,link_color_func=lambda x: link_cols[x],ax=ax_dend_2)\n",
    "\n",
    "    ls = []\n",
    "    for leaf, leaf_color in zip(plt.gca().get_xticklabels(), dd[\"leaves_color_list\"]):\n",
    "        leaf.set_color(cols[int(leaf.get_text())])\n",
    "        ls.append(int(leaf.get_text()))\n",
    "\n",
    "    ax_dend_2.set_ylabel('Haplotype age/generations',fontsize=24)\n",
    "    \n",
    "    \n",
    "    # Save dendrogram\n",
    "    output = 'accurate_' + file + '.pdf'\n",
    "    plt.savefig(output)  \n",
    "        \n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
