{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering simulated sequences using TMRCA using an array job (not working)\n",
    "\n",
    "Name: CQS21 \n",
    "FYP 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PBS_ARRAY_INDEX'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m array_index \n\u001b[1;32m---> 35\u001b[0m array_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPBS_ARRAY_INDEX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;66;03m# HPC ppl suggested using os module, gets it as a str, need to int() it\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#instead of using sys.argv[1] which takes the first argument passed\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m (array_index)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[1;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'PBS_ARRAY_INDEX'"
     ]
    }
   ],
   "source": [
    "# installing packages to my python environment\n",
    "# NOTE: my pip is v22.0.4 but I cant seem to update to the newest 24.0\n",
    "\n",
    "#!pip install zarr\n",
    "#!pip install scipy\n",
    "#!pip install scikit-allel\n",
    "#!pip install matplotlib\n",
    "#!pip install tqdm\n",
    "#!pip install dask\n",
    "#!pip install seaborn\n",
    "#!pip install tskit\n",
    "#!pip install msprime\n",
    "#!pip install pyslim\n",
    "\n",
    "##conda install? matplotlib \n",
    "# do not do pip after using conda \n",
    "\n",
    "global genome_length\n",
    "genome_length =70000\n",
    "global mutation\n",
    "mutation = int((genome_length+1)/2) \n",
    "global threshold\n",
    "threshold = 0.87  #setting threshold to calculate trough points in each haplotype\n",
    "global points\n",
    "points=280\n",
    "global window \n",
    "window=600\n",
    "global sample_size\n",
    "sample_size = 100\n",
    "global no_haplotypes\n",
    "no_haplotypes = sample_size*2\n",
    "\n",
    "import os\n",
    "global array_index \n",
    "array_index = int(os.environ['PBS_ARRAY_INDEX']) # HPC ppl suggested using os module, gets it as a str, need to int() it\n",
    "#instead of using sys.argv[1] which takes the first argument passed\n",
    "print (array_index)\n",
    "\n",
    "global seed\n",
    "seed = int(os.environ['seed'])\n",
    "print (seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import zarr\n",
    "import allel   \n",
    "#idk why but my allel doesnt seem to load?? even though ive definitely installed it as scikit-allel\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.spatial\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "from dask.delayed import delayed\n",
    "from dask.base import compute\n",
    "from itertools import combinations\n",
    "import seaborn as sns\n",
    "#import tskit\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEE HPC STEPS.ipynb\n",
    "\n",
    "Step 1. Generate burn in simulations establishing nucleotide diversity across a range of parameters (redo theo's work but w updated vgsc parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Conduct 27 variations of partial soft sweep simulations and output vcf files using the HPC facility. Complete x number of repeats.\n",
    "\n",
    "Size of population, N: 100, 1000, 10000\n",
    "\n",
    "Mutation rate, Î¼ : (0.1/4N), (1/4N), (5/4N)\n",
    "\n",
    "Recombination rate, r : (0.1/4N), (1/4N), (5/4N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking .tree outputs\n",
    "#import tskit\n",
    "#tskit.load(file=\"C:/CHEYANNE STUFF/ICL Biology/Year 3/Final year project/burnin_no.19_early.trees\")\n",
    "#tskit.load(file=\"C:/CHEYANNE STUFF/ICL Biology/Year 3/Final year project/burnin_no.19_early_10.trees\")\n",
    "\n",
    "\n",
    "# install pyslim !!! issue here involving msprime? pyslim cannot be imported/downloaded\n",
    "#import pyslim\n",
    "\n",
    "# read into pyslim and calculate nucleotide diversity and compare against expected nucleotide diversity (theta)\n",
    "# check that it exceeds 10% and not more than 20%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Read in VCF Files from soft sweep simulations, extract haplotypes using scikit_allel\n",
    "\n",
    "Working with 27 simulation variants and repeats of each variant (100?)\n",
    "\n",
    "but why only look at haplotypes of first 200 individuals? and why calculate sample frequency over specifically 400 haplotypes? is it linked to the 200 individuals = 400 genomes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#OUT: 10000 10000 GV 20\n",
      "##fileformat=VCFv4.2\n",
      "##fileDate=20160613\n",
      "##source=SLiM\n",
      "##slimGenomePedigreeIDs=4999898,4999899,...\n",
      "##INFO=<ID=MID,Number=1,Type=Integer,Description=\"Mutation ID in SLiM\">\n",
      "##INFO=<ID=S,Number=1,Type=Float,Description=\"Selection Coefficient\">\n",
      "##INFO=<ID=DOM,Number=1,Type=Float,Description=\"Dominance\">\n",
      "##INFO=<ID=PO,Number=1,Type=Integer,Description=\"Population of Origin\">\n",
      "##INFO=<ID=TO,Number=1,Type=Integer,Description=\"Tick of Origin\">\n",
      "##INFO=<ID=MT,Number=1,Type=Integer,Description=\"Mutation Type\">\n",
      "##INFO=<ID=AC,Number=1,Type=Integer,Description=\"Allele Count\">\n",
      "##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">\n",
      "##INFO=<ID=MULTIALLELIC,Number=0,Type=Flag,Description=\"Multiallelic\">\n",
      "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n",
      "#CHROM POS ID REF ALT QUAL FILTER INFO FORMAT i0 i1 i2 i3 i4 i5...\n",
      "1 1309 . A T 1000 PASS\n",
      "MID=987550;S=0;DOM=0.5;PO=2;TO=9404;MT=1;AC=11;DP=1000 GT 0|1 0|0 0|1...\n",
      "1 5995 . A T 1000 PASS\n",
      "MID=987764;S=0;DOM=0.5;PO=1;TO=9415;MT=1;AC=11;DP=1000 GT 0|1 0|0 0|1...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# load example vcf from SLIM\n",
    "#with open('example.vcf', mode='r') as vcf:\\\n",
    "\n",
    "    #print(vcf.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prev student: Like Hamming distance code, this was also taken from Anushka Thawani. Adaptations were made to this on the \n",
    "#high-performance computer using shell script, but this could not be represented.\n",
    "def convert(file):\n",
    "    '''\n",
    "    This function extracts haplotypes sequences from a vcf file \n",
    "    Adapted from: http://alimanfoo.github.io/2018/04/09/selecting-variants.html \n",
    "    \n",
    "    Arguments:\n",
    "        file: name of vcf file (from SLiM soft sweep simulation)\n",
    "        \n",
    "    Returns:\n",
    "        ht: haplotype sequences for 200 individuals\n",
    "        samp_freq: frequency of sweep mutation in sample\n",
    "        cols: used to color dendrogram\n",
    "        pos: \n",
    "\n",
    "    '''\n",
    "    \n",
    "    v = file + '.vcf'\n",
    "    z = file + '.zarr'\n",
    "    slim_sim_data = allel.read_vcf(v, fields='*')\n",
    "    allel.vcf_to_zarr(v, z, fields='*', overwrite=True)\n",
    "    data = zarr.open_group(z, mode='r')\n",
    "    \n",
    "    # Stores the ID and genomic position of each variant \n",
    "    pos = allel.SortedIndex(data['variants/POS'])             #KEEPS THROWING KEY ERROR meaning pos start and stop is the same! Why!\n",
    "    \n",
    "    # Extract genotypes for the first 200 individuals and convert to haplotypes\n",
    "    gt = data['calldata/GT'][:,0:200] \n",
    "    ht = allel.GenotypeArray(gt).to_haplotypes()\n",
    "    \n",
    "    mutation = int((genome_length+1)/2) # position of sweep mutation\n",
    "    \n",
    "    \n",
    "    # Output the frequency of the sweep mutation in the sample\n",
    "    contains_sweep = pos.locate_range(mutation,mutation) #finds index of sweep mutation in the array\n",
    "    sweep = ht[contains_sweep]                           # saves the haplotypes containing the sweep in the variable sweep\n",
    "    sweep = np.sum(sweep, axis =0)                       #sums up mutation occurences in each haplotypes\n",
    "    \n",
    "    samp_freq = np.sum(sweep)/200  # finds freq in the entire sample of 100 individuals, 200 haplotypes\n",
    "    \n",
    "    \n",
    "    # This dictionary is used later to color the dendrogram branches according to whether or not the \n",
    "    # corresponding sequence contains the sweep mutation\n",
    "    cols = {}\n",
    "    for i in range(200):\n",
    "        if sweep[i]:\n",
    "            cols[i] = \"#FF0000\" \n",
    "        else:\n",
    "            cols[i] = \"#808080\"\n",
    "    \n",
    "    return ht, pos, samp_freq, cols, sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Calculate Hij (S or homozygosity) for all pairs of haplotypes\n",
    "make a function \n",
    "homozygosity = (no. of SNPs/ length of window, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_homozygosity(ht, pos):\n",
    "    '''\n",
    "    This function calculates the sliding homozygosity for all pairs of haplotypes.\n",
    "    \n",
    "    Arguments:\n",
    "    window: length of sliding window\n",
    "    len_genome : length of genome \n",
    "    ht : vector?  of haplotype sequences from convert() function in previous codeblock\n",
    "    pos: position of variants from convert() function in previous codeblock\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    homozygosities:  homozygosity of all haplotypes in ht in a array(?)\n",
    "    '''\n",
    "    # Make empty vectors\n",
    "    homozygosities = []\n",
    "    \n",
    "    #iterate through all nucleotides in the genome\n",
    "    for x in range(0, genome_length):\n",
    "        #define nt ranges \n",
    "        start  = x\n",
    "        end = x + window\n",
    "        # locate the position of region around a variant Nt\n",
    "        region = pos.locate_range(start,end) \n",
    "        haplotype_region = ht [region]\n",
    "\n",
    "        #use allel.pairwise distance\n",
    "        pairwise_dist = allel.pairwise_distance(haplotype_region, metric = 'hamming' , chunked=True, blen=None) #can i chunk = true to speed up computation? \n",
    "        homozygosities = pairwise_dist/window\n",
    "\n",
    "    return homozygosities\n",
    "\n",
    "\n",
    "\n",
    "## TO DO streamlining: previous students code say to add a if condition (region =/= prev region to speed up code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Calculate Lij (shared haplotype length) from Hij by finding width at half maximum homozygosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#half_max_homozygosity = (max(homozygosities) + min(homozygosities))/2\n",
    "\n",
    "#Defining function to find troughs, to be used in calculating Lij in another function\n",
    "def finding_troughs(smooth, pos):\n",
    "    '''\n",
    "    This function finds troughs for a pair of haplotype sequences. \n",
    "    Note: threshold set to 0.87\n",
    "    \n",
    "    Arguments:\n",
    "        sliding homozygosity: smoothed sliding window homozygosity for all pairs of sequences\n",
    "        sweeploc: position of sweep mutation in genome (same as previous function)\n",
    "        \n",
    "    Returns:\n",
    "        lower: position of breakpoint left of the sweep site\n",
    "        upper: position of breakpoint right of the sweep site\n",
    "        SHL: shared haplotype length\n",
    "    '''\n",
    "    global threshold\n",
    "    threshold = 0.87  #setting threshold to calculate trough points in each haplotype\n",
    "\n",
    "    #finding troughs\n",
    "    troughs = scipy.signal.find_peaks(-smooth) #- sign inverts the graph so the 'peaks' are our troughs\n",
    "    troughs = troughs[0]     # Indexes of all troughs\n",
    "    troughs = troughs[smooth[troughs] < threshold]   # Extract troughs where homozygosity<threshold\n",
    "    \n",
    "    #finds the peaks\n",
    "    peaks = scipy.signal.find_peaks(smooth)\n",
    "    peaks = peaks[0]  #indexes peaks\n",
    "    \n",
    "    # Find positions of troughs flanking sweep site\n",
    "    bp = np.searchsorted(troughs,pos)  #search sorted finds index of position where mutation should be inserted in order to maintain the same order\n",
    "    lower = troughs[bp - 1] #index of sweep site -1\n",
    "    upper = troughs[bp]  #index of sweep site\n",
    "    \n",
    "    # Find the average peak position around the sweep site\n",
    "    highest = peaks[(peaks >= lower) & (peaks <= upper)]\n",
    "    if highest.size != 0:\n",
    "        highest = np.mean(highest)\n",
    "    else: \n",
    "        highest = (lower+upper)/2\n",
    "    \n",
    "    \n",
    "    lower = (lower+highest)/2\n",
    "    upper = (upper+highest)/2\n",
    "\n",
    "    SHL = upper - lower\n",
    "    \n",
    "    return int(lower), int(upper), SHL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function using each haplotype pair to return SHL and find lower and upper limits of SHL\n",
    "# uses finding_troughs()\n",
    "def find_breakpoint(haplotype_pair):\n",
    "    '''\n",
    "    For a pair of sequences, this function smoothes the sliding homozygosity and returns the SHL\n",
    "    Arguments:haplotype_pair\n",
    "        haplotype_pair: a pair of haplotype sequences\n",
    "        \n",
    "    Returns:\n",
    "        lower: position of breakpoint left of the sweep site\n",
    "        upper: position of breakpoint right of the sweep site\n",
    "        SHL: shared haplotype length\n",
    "    '''\n",
    "    \n",
    "    mutation_pos = mutation \n",
    "    smooth = gaussian_filter1d(haplotype_pair, points)\n",
    "    try:\n",
    "        lower, upper, SHL = finding_troughs(smooth, mutation_pos)\n",
    "    except IndexError:\n",
    "        lower = -1.3\n",
    "        upper = -1.3\n",
    "        SHL = -1.3\n",
    "        \n",
    "    return lower, upper, SHL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6. Calculate Tij (Time to common ancestor) from Lij and Kij (which is no. of SNPs) on each shared length (haplotype)\n",
    "\n",
    "ð_ðð=(ð_ðð+1)/(2â_ðð (ð+ð))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Kij (No. of SNPs in each SHL)\n",
    "\n",
    "def calculating_kij(gts,ht,result_find_breakpoint,pos):\n",
    "    '''\n",
    "    This function finds the number of SNPs over the shared haplotype length for all pairs of haplotype sequences\n",
    "    \n",
    "    Arguments:\n",
    "        gts: number of haplotype pairs\n",
    "        ht: haplotype sequnces\n",
    "        result_find_breakpoint: output from find_breakpoint function\n",
    "        \n",
    "    Returns:\n",
    "        diffs: number of SNPs for all pairs of haplotype sequences\n",
    "        \n",
    "    '''\n",
    "    pairwise = []\n",
    "    for combo in combinations(list(range(0,no_haplotypes)), 2): \n",
    "        pairwise.append(combo)\n",
    "\n",
    "    diffs = np.empty(shape=(gts),dtype=np.float32)\n",
    "    for i in range(gts):\n",
    "        pair = ht[:,pairwise[i]]\n",
    "        try:\n",
    "            start = result_find_breakpoint[i,1]\n",
    "            stop = result_find_breakpoint[i,2]\n",
    "\n",
    "            window_pos = pos.locate_range(start, stop)\n",
    "            window = pair[window_pos]\n",
    "\n",
    "            d = allel.pairwise_distance(window, metric = \"hamming\")\n",
    "\n",
    "            diffs[i]=d \n",
    "\n",
    "        except KeyError:\n",
    "            diffs[i]=-1.3 \n",
    "    \n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Tij, Time to Common Ancestor (TMRCA) using mutation rate and number of SNPs\n",
    "# ð_ðð=(ð_ðð+1)/(2â_ðð (ð+ð))\n",
    "#for array number x:\n",
    "    #read in vcf file\n",
    "    # read in population size, mu, r from population parameters csv file\n",
    "\n",
    "\n",
    "def calculating_Tij(array_index, seed, cutoff):\n",
    "    '''\n",
    "    This function calculates Tij, Time to Common Ancestor (TMRCA) using mutation rate and number of SNPs for each haplotype pair.\n",
    "    It uses the convert() to convert files from vcf \n",
    "    It uses the sliding_heterozygosity() to calculate heterozygosity in the window for all pairs of haplotypes\n",
    "\n",
    "    \n",
    "    Arguments:\n",
    "        array_index: array index or combination number of simulation\n",
    "        seed: seed number of simulation\n",
    "        cutoff: 40 % allele freq for rdl and 80% for vgsc\n",
    "\n",
    "        \n",
    "        genome_length: length of genome (in SLiM simulation)             ##can loop?\n",
    "        ht : haplotypes (what variable structure?)\n",
    "\n",
    "        window: length of sliding window\n",
    "        threshold: threshold above which troughs are ignored\n",
    "        points: number of points to use for 1D-gaussian filter (see scipy documentation)\n",
    "        \n",
    "    Returns:\n",
    "        Tij: Time to Common Ancestor (TMRCA)\n",
    "  \n",
    "    '''\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    ## read in population size, mu, r from population parameters csv\n",
    "    parameters = pd.read_csv('C:/CHEYANNE STUFF/ICL Biology/Year 3/Final year project/Estimating-effective-population-size-from-resistance-loci/simulations/parameter_combinations.csv')\n",
    "    index = array_index +1\n",
    "    global pop_size\n",
    "    pop_size = parameters.iloc[index]['N']\n",
    "    global mu\n",
    "    mu = parameters.iloc[index]['Mutation Rate']\n",
    "    global r\n",
    "    r = parameters.iloc[index]['Recombination Rate']\n",
    "\n",
    "\n",
    "    # for array number:\n",
    "        #read in vcf file\n",
    "    file = \"C:/CHEYANNE STUFF/ICL Biology/Year 3/Final year project/Estimating-effective-population-size-from-resistance-loci/Post Simulation/\" + str(array_index) + \"_\"+ str(seed) +\"_\"+ str(cutoff)  #\".vcf\" added by convert()\n",
    "    # Extract haplotype sequences from .vcf file\n",
    "    ht, pos, samp_freq, cols, sweep = convert(file)\n",
    "\n",
    "    \n",
    "    # Calculate sliding homozygosity for all pairs of haplotype sequences\n",
    "    gts = int((no_haplotypes*(no_haplotypes-1))/2)\n",
    "    homozygosities = sliding_homozygosity(ht, pos)\n",
    "\n",
    "    \n",
    "    # Find SHL for all pairs of haplotype sequences \n",
    "    hom_dask = dask.array.from_array(homozygosities, chunks=(genome_length,1)) # type: ignore # creates a dask array\n",
    "    homozygosities = []\n",
    "    results = dask.array.apply_along_axis(find_breakpoint, 0, hom_dask) # type: ignore #applies find_breakpoint() along the array\n",
    "    results_computed = results.compute()\n",
    "\n",
    "    # Manipulating the dataframe to make it easier to process\n",
    "    results_computed = np.transpose(results_computed)\n",
    "    index = np.asarray(range(0,gts))\n",
    "    index = np.expand_dims(index, axis=0)\n",
    "    results_computed_1 = np.concatenate((index.T, results_computed), axis=1)\n",
    "    \n",
    "    \n",
    "    # Calculate the TMRCA from the SHLs and number of SNPs\n",
    "    recombination_rate = r/(2*pop_size)\n",
    "    mu = mu/(100*pop_size)\n",
    "    shls = results_computed_1[:,3]   # SHLs for all pairs of haplotype sequences \n",
    "    shls[shls<=0] = genome_length\n",
    "    diffs = calculating_kij(gts, ht, results_computed_1, pos)  # SNPs for all pairs of haplotype sequences \n",
    "    Tij = (1+(diffs*shls))/(2*shls*(recombination_rate + mu)) # TMRCA metric for all pairs of haplotype sequences \n",
    "\n",
    "    \n",
    "    # Remove negative and non-integer TMRCA values\n",
    "    impute = np.nanmean(Tij)        #impute is the mean of SNP array without any NAN values\n",
    "    x = np.isfinite(Tij)            #x is a boolean mask array of only finite values (cannot be infinite or NAN)\n",
    "    for i in np.where(x == 0)[0]:   #for all indices where there is a non-finite number:\n",
    "        Tij[i] = impute             #replace NaN with inpute value\n",
    "    Tij[Tij<=0] = impute            # replace all negative numbers wih inpute\n",
    "\n",
    "    return Tij, cols, pop_size, mu, r\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7. Plotting TMRCA dendrogram \n",
    "- prev student ran into problems here with generating the dendrogram coloured tips\n",
    "- Untested to see if I have resolved previous student problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analysis(array_index, seed, cutoff): \n",
    "\n",
    "    '''\n",
    "    This function plots a dendrogram and colours red the tips that have the sweep mutations. \n",
    "    It uses calculating_Tij().\n",
    "    Only the output vcf from the SLIM simulation is input.\n",
    "    global variables genome_length, window, threshold, points being used\n",
    "\n",
    "    Arguments:\n",
    "    array_index: array index or combination number of simulation\n",
    "    seed: seed number of simulation\n",
    "    cutoff: 40 % allele freq for rdl and 80% for vgsc\n",
    "    \n",
    "    global variables/ variables from sub functions:\n",
    "    window: length of sliding window\n",
    "    threshold: threshold above which troughs are ignored\n",
    "    points: number of points to use for 1D-gaussian filter (see scipy documentation)\n",
    "    cols: colours haplotype branches red if they have sweep mutation. From convert().\n",
    "\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    output dendrogram in pdf\n",
    "    '''\n",
    "    #all convert(), etc etc to end up with tij\n",
    "    Tij, cols, pop_size, mu, r = calculating_Tij(array_index, seed, cutoff)\n",
    "    \n",
    "    # Hierachical Clustering, store in Z\n",
    "    Z = sch.linkage(Tij, method = 'average') #why do we use the Farthest Point Algorithm? changed to average (UPGMA) , check after\n",
    "    \n",
    "\n",
    "    ## Plot dendrogram without colouring branches\n",
    "    # updating matplotlib font settings\n",
    "    matplotlib.rcParams.update({'font.size': 24})\n",
    "    fig = plt.figure(figsize=(30, 12))\n",
    "    gs = matplotlib.gridspec.GridSpec(2, 1, hspace=0.1, wspace=1, height_ratios=(1,1)) # type: ignore\n",
    "\n",
    "    ax_dend = fig.add_subplot(gs[0, 0])\n",
    "    sns.despine(ax=ax_dend, offset=5, bottom=True, top=True)\n",
    "    dd = sch.dendrogram(Z,color_threshold=0,above_threshold_color='#808080',ax=ax_dend) # if above colour threshold, set colour to grey \n",
    "\n",
    "    ls = []\n",
    "    for leaf, leaf_color in zip(plt.gca().get_xticklabels(), dd[\"leaves_color_list\"]):   #leaves_color_list is A list of color names. The kâth element represents the color of the kâth leaf.\n",
    "        leaf.set_color(cols[int(leaf.get_text())])\n",
    "        ls.append(int(leaf.get_text()))\n",
    "\n",
    "    ax_dend.set_ylabel('Haplotype age/generations',fontsize=24)\n",
    "    ax_dend.set_title('Haplotype clusters',fontsize=24)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot dendrogram and colour branches\n",
    "    ax_dend_2 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    dflt_col = \"#808080\"\n",
    "    \n",
    "    link_cols = {}\n",
    "    for i, i12 in enumerate(Z[:,:2].astype(int)):\n",
    "        c1, c2 = (link_cols[x] if x > len(Z) else cols[x] for x in i12)\n",
    "        link_cols[i+1+len(Z)] = c1 if c1 == c2 else dflt_col\n",
    "\n",
    "    sns.despine(ax=ax_dend_2, offset=5, bottom=True, top=True)\n",
    "    dd = sch.dendrogram(Z,link_color_func=lambda x: link_cols[x], ax=ax_dend_2)\n",
    "\n",
    "    ls = []\n",
    "    for leaf, leaf_color in zip(plt.gca().get_xticklabels(), dd[\"leaves_color_list\"]):\n",
    "        leaf.set_color(cols[int(leaf.get_text())])\n",
    "        ls.append(int(leaf.get_text()))\n",
    "\n",
    "    ax_dend_2.set_ylabel('Haplotype age/generations',fontsize=24)\n",
    "    \n",
    "    \n",
    "    # Save dendrogram\n",
    "    output_directory = '/dendrograms/' \n",
    "    output = output_directory + 'dendrogram_' + str(array_index) + \"_\"+ str(seed) +\"_\"+ str(cutoff) +'.pdf' \n",
    "    print (output)\n",
    "    plt.savefig(output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(35001, 35001)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1811758731\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#run analysis\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43manalysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#analysis(array_index, seed, 80)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 26\u001b[0m, in \u001b[0;36manalysis\u001b[1;34m(array_index, seed, cutoff)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mThis function plots a dendrogram and colours red the tips that have the sweep mutations. \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mIt uses calculating_Tij().\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03moutput dendrogram in pdf\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#all convert(), etc etc to end up with tij\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m Tij, cols, pop_size, mu, r \u001b[38;5;241m=\u001b[39m \u001b[43mcalculating_Tij\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Hierachical Clustering, store in Z\u001b[39;00m\n\u001b[0;32m     29\u001b[0m Z \u001b[38;5;241m=\u001b[39m sch\u001b[38;5;241m.\u001b[39mlinkage(Tij, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#why do we use the Farthest Point Algorithm? changed to average (UPGMA) , check after\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 50\u001b[0m, in \u001b[0;36mcalculating_Tij\u001b[1;34m(array_index, seed, cutoff)\u001b[0m\n\u001b[0;32m     48\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/CHEYANNE STUFF/ICL Biology/Year 3/Final year project/Estimating-effective-population-size-from-resistance-loci/Post Simulation/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(array_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(seed) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(cutoff)  \u001b[38;5;66;03m#\".vcf\" added by convert()\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Extract haplotype sequences from .vcf file\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m ht, pos, samp_freq, cols, sweep \u001b[38;5;241m=\u001b[39m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenome_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Calculate sliding homozygosity for all pairs of haplotype sequences\u001b[39;00m\n\u001b[0;32m     54\u001b[0m gts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((no_haplotypes\u001b[38;5;241m*\u001b[39m(no_haplotypes\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(file, genome)\u001b[0m\n\u001b[0;32m     33\u001b[0m mutation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((genome_length\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# position of sweep mutation\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Output the frequency of the sweep mutation in the sample\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m contains_sweep \u001b[38;5;241m=\u001b[39m \u001b[43mpos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocate_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmutation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmutation\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#finds index of sweep mutation in the array\u001b[39;00m\n\u001b[0;32m     38\u001b[0m sweep \u001b[38;5;241m=\u001b[39m ht[contains_sweep]                           \u001b[38;5;66;03m# saves the haplotypes containing the sweep in the variable sweep\u001b[39;00m\n\u001b[0;32m     39\u001b[0m sweep \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(sweep, axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)                       \u001b[38;5;66;03m#sums up mutation occurences in each haplotypes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\allel\\model\\ndarray.py:3611\u001b[0m, in \u001b[0;36mSortedIndex.locate_range\u001b[1;34m(self, start, stop)\u001b[0m\n\u001b[0;32m   3608\u001b[0m     stop_index \u001b[38;5;241m=\u001b[39m bisect\u001b[38;5;241m.\u001b[39mbisect_right(\u001b[38;5;28mself\u001b[39m, stop)\n\u001b[0;32m   3610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_index \u001b[38;5;241m-\u001b[39m start_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(start, stop)\n\u001b[0;32m   3613\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(start_index, stop_index)\n\u001b[0;32m   3614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loc\n",
      "\u001b[1;31mKeyError\u001b[0m: (35001, 35001)"
     ]
    }
   ],
   "source": [
    "#test \n",
    "\n",
    "#run analysis\n",
    "analysis(array_index, seed, 40)\n",
    "#analysis(array_index, seed, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Estimate nhat by counting number of origins in dendrogram\n",
    "I think has to be done manually... have not solved how to get it done with python\n",
    "\n",
    "do it for each and then record in the results.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count number of origins (haplotypes) for all files and save in a list\n",
    "\n",
    "def sum_origins (number_of_files, files):\n",
    "    '''\n",
    "    takes the number of simulations, the .vcf files and \n",
    "    calculates the number of origins per simulation and saves it in a list.\n",
    "\n",
    "\n",
    "    returns:\n",
    "\n",
    "    '''\n",
    "    # search if can use sch.dendrogram to get number of haplotype origins \n",
    "    origins = []\n",
    "    for i in range (number_of_files):\n",
    "        read.file ( file + i + .TREES) as file: \n",
    "\n",
    "            #if _________ =___________:\n",
    "                #calculate  number of origins as file_origins\n",
    "        #attach file_origins to origins\n",
    "    return print (origins)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Calculate Ne hat by maximum likelihood estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "#plot all estimated numbers of origins\n",
    "\n",
    "# find a function that plots and finds maximum likelihood and the 95% confidence intervals\n",
    "# calculating confidence interval by \n",
    "# 1. log ne hat and find maximum which should be the maximum of the graph as well\n",
    "# average(origins) ?\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def NeHat():\n",
    "    '''\n",
    "    This function plots the number of independent gene loci/origins (L) against population size (N). It also tells us NeHat (the mean), and the 95% confidence intervals.\n",
    "    Do we just do the confidence intervals per N? \n",
    "    \n",
    "    Arguments:\n",
    "        csv_file_path: filepath of the results.csv used to manually record down the number of origins per simulation\n",
    "        \n",
    "    Returns:\n",
    "        NeLow: lower point of 95% confidence intervals of NeHat being correct\n",
    "        NeHigh: higher point of 95% confidence intervals of NeHat being correct\n",
    "        NeHat: NeHat (at ln(Lmax))\n",
    "\n",
    "    ''' \n",
    "    #load modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.stats import norm\n",
    "    import matplotlib.pyplot as plt\n",
    "    import scipy\n",
    "\n",
    "    #load csv\n",
    "    origins = pd.read_csv('simulations/results.csv')\n",
    "\n",
    "\n",
    "    # log the heterozygosity (ln(L))\n",
    "    origins[\"ln(L)\"] = np.log(origins['origins'])\n",
    "    \n",
    "    # At max ln(L), is Ne Hat\n",
    "    #Lmax = origins['origins'].max()\n",
    "    #ln_Lmax = np.log(Lmax) \n",
    "    \n",
    "    # at [ln(Lmax)-2], Nelow and NeHigh are the confidence intervals\n",
    "    #confidence_cutoff = ln_Lmax -2 \n",
    "    #NeLow =       ##how to find best fit line and intersection?\n",
    "\n",
    "    \n",
    "    # load input variables from a file\n",
    "    data =origins[['N','ln(L)']].copy()\n",
    "\n",
    "    #subset by N and then loop through each \n",
    "    size_100 = data[(data[\"N\"] == 100)]\n",
    "    size_1000 = data[(data[\"N\"] == 1000)]\n",
    "    size_10000 = data[(data[\"N\"] == 10000)]\n",
    "    print (size_100)                                            \n",
    "    \n",
    "    # Fit a normal distribution to the data:\n",
    "    mean, std = norm.fit(size_100)\n",
    "    NeHat = mean #NeHat is the mean\n",
    "\n",
    "    # get confidence intervals\n",
    "    ci = scipy.stats.norm.interval(0.95, loc=mean, scale=1) #gets Confidence interval with equal areas around the mean\n",
    "    #print(ci)\n",
    "\n",
    "    # Plot \n",
    "    plt.plot(size_100['N'], size_100['ln(L)'] )\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the PDF.\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 1)\n",
    "    p = norm.pdf(x, mean, std)\n",
    "    plt.plot(x, p, 'k', linewidth=2)\n",
    "    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mean, std)\n",
    "    # confidence interval left line\n",
    "    one_x12, one_y12 = [ci[0],ci[0]], [0, 20]\n",
    "    # confidence interval right line\n",
    "    two_x12, two_y12 = [ci[1],ci[1]], [0, 20]\n",
    "    plt.plot(one_x12, one_y12, two_x12, two_y12, marker = 'o')\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    NeLow = ci[0]\n",
    "    NeHigh = ci[1]\n",
    "    print ('NeLow, NeHigh:' + str(NeLow) + \",\" + str(NeHigh))\n",
    "\n",
    "    # plot to look at confidence intervals, NeHat\n",
    "    #Lmax = max (df['Heterozygosity'])\n",
    "    #ln_Lmax = np.log(Lmax) \n",
    "    #plt.plot('cycle','Heterozygosity', data = df)\n",
    "    #plt.axhline(y =ln_Lmax, color='r', linestyle='-') #ln_Lmax\n",
    "    #plt.axhline(y=confidence_cutoff, color='r', linestyle='-') #confidence interval cutoff\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "    return NeLow, NeHigh, NeHat\n",
    "\n",
    "NeHat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
